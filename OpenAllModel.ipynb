{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv1LIk5pwvgk",
        "outputId": "a954dd9f-628b-4612-8065-6cf419db6f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for SubCategory...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for SubCategory:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': False}\n",
            "Accuracy for SubCategory: 0.8356532849169525\n",
            "Precision for SubCategory: 0.8348256877124407\n",
            "Recall for SubCategory: 0.8356532849169525\n",
            "F1 Score for SubCategory: 0.8288257455616973\n",
            "Confusion Matrix for SubCategory:\n",
            "[[ 2  0  0 ...  0  0  0]\n",
            " [ 0  3  1 ...  0  0  0]\n",
            " [ 0  0  7 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 45]]\n",
            "\n",
            "Training model for Category...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Category:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': True}\n",
            "Accuracy for Category: 0.8597269854078408\n",
            "Precision for Category: 0.8545040616810186\n",
            "Recall for Category: 0.8597269854078408\n",
            "F1 Score for Category: 0.8543691050203744\n",
            "Confusion Matrix for Category:\n",
            "[[151   1   0 ...   1   0   8]\n",
            " [  1  10   0 ...   0   0   0]\n",
            " [  0   0  25 ...   0   0   0]\n",
            " ...\n",
            " [  4   0   0 ... 149   0   0]\n",
            " [  2   0   0 ...   1   9   0]\n",
            " [  6   0   0 ...   0   0 227]]\n",
            "\n",
            "Training model for Company...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Company:  {'clf__alpha': 0.1, 'vect__ngram_range': (3, 5), 'vect__use_idf': True}\n",
            "Accuracy for Company: 0.8890457938269114\n",
            "Precision for Company: 0.8903885242003067\n",
            "Recall for Company: 0.8890457938269114\n",
            "F1 Score for Company: 0.8658004306971294\n",
            "Confusion Matrix for Company:\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  1  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  1]\n",
            " [ 0  0  0 ...  0  0 27]]\n",
            "\n",
            "Training model for Brand...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Brand:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': False}\n",
            "Accuracy for Brand: 0.8818505816690202\n",
            "Precision for Brand: 0.8997593660895931\n",
            "Recall for Brand: 0.8818505816690202\n",
            "F1 Score for Brand: 0.880136233834143\n",
            "Confusion Matrix for Brand:\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 1  0  0 ...  0  0  0]\n",
            " [ 0  0  2 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 39]]\n",
            "Predicted SubCategory for 'pepsi500': CARBONATED SOFT DRINKS\n",
            "Predicted Category for 'pepsi500': BEVERAGES\n",
            "Predicted Company for 'pepsi500': PEPSICO\n",
            "Predicted Brand for 'pepsi500': PEPSI\n"
          ]
        }
      ],
      "source": [
        "# import json\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import string\n",
        "# from tqdm import tqdm\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk import word_tokenize\n",
        "# import re\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "# import numpy as np\n",
        "\n",
        "# # Set random seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "# tqdm.pandas()\n",
        "\n",
        "# # Download necessary NLTK data\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# # Load and preprocess the data\n",
        "# df = pd.read_csv(\"OpenItemAll.csv\")\n",
        "# # df = df[~df['SubCategory'].isin(df['SubCategory'].value_counts()[df['SubCategory'].value_counts() < 2].index)]\n",
        "# # df = df[~df['Category'].isin(df['Category'].value_counts()[df['Category'].value_counts() < 2].index)]\n",
        "# df['SubCategory'] = df['SubCategory'].str.upper().str.strip().replace({'MEN DEODORANT': 'MEN DEODORANTS'})\n",
        "# df['SubCategory'] = df['SubCategory'].str.upper().str.strip().replace({'SWEET CAKE': 'SWEET CAKES'})\n",
        "# df['ProductName'] = df['ProductName'].fillna('')  # or df['ProductName'].astype(str)\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     text = text.lower()\n",
        "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "#     tokens = word_tokenize(text)\n",
        "#     tokens = [word for word in tokens if word not in stop_words]\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#     return ' '.join(tokens)\n",
        "\n",
        "# df['ProductName'] = df['ProductName'].apply(preprocess_text)\n",
        "\n",
        "# # Define features and target columns\n",
        "# X = df['ProductName']\n",
        "# target_columns = ['SubCategory', 'Category', 'Company', 'Brand']\n",
        "\n",
        "# # Prepare dictionaries to store results for each target column\n",
        "# models = {}\n",
        "# encoders = {}\n",
        "# grid_search_results = {}\n",
        "\n",
        "# # Loop over each target column\n",
        "# for target in target_columns:\n",
        "#     print(f\"\\nTraining model for {target}...\")\n",
        "\n",
        "#     # Encode the target variable\n",
        "#     encoder = LabelEncoder()\n",
        "#     y = encoder.fit_transform(df[target])\n",
        "\n",
        "#     # Store the encoder for later use\n",
        "#     encoders[target] = encoder\n",
        "\n",
        "#     # Split the data\n",
        "#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
        "\n",
        "#     # Define the pipeline\n",
        "#     pipeline_nb = Pipeline([\n",
        "#         ('vect', TfidfVectorizer(analyzer='char', stop_words='english')),\n",
        "#         ('clf', MultinomialNB()),\n",
        "#     ])\n",
        "\n",
        "#     # Define the parameter grid\n",
        "#     param_grid = {\n",
        "#         'vect__ngram_range': [(1, 3), (2, 4), (3, 5)],\n",
        "#         'vect__use_idf': [True, False],\n",
        "#         'clf__alpha': [0.1, 0.01, 0.001]\n",
        "#     }\n",
        "\n",
        "#     # Perform grid search\n",
        "#     grid_search_nb = GridSearchCV(pipeline_nb, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "#     grid_search_nb.fit(x_train, y_train)\n",
        "\n",
        "#     # Store the trained model\n",
        "#     models[target] = grid_search_nb\n",
        "#     grid_search_results[target] = grid_search_nb.best_params_\n",
        "\n",
        "#     # Make predictions\n",
        "#     y_pred_nb = grid_search_nb.predict(x_test)\n",
        "\n",
        "#     # Calculate and print metrics\n",
        "#     accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "#     precision_nb = precision_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "#     recall_nb = recall_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "#     f1_nb = f1_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "#     conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "#     unique_classes = np.unique(np.concatenate((y_test, y_pred_nb)))\n",
        "#     target_names = encoder.inverse_transform(unique_classes)\n",
        "#     # class_report_nb = classification_report(y_test, y_pred_nb, labels=unique_classes, target_names=target_names, zero_division=1)\n",
        "\n",
        "#     print(f\"Best parameters for {target}: \", grid_search_nb.best_params_)\n",
        "#     print(f\"Accuracy for {target}: {accuracy_nb}\")\n",
        "#     print(f\"Precision for {target}: {precision_nb}\")\n",
        "#     print(f\"Recall for {target}: {recall_nb}\")\n",
        "#     print(f\"F1 Score for {target}: {f1_nb}\")\n",
        "#     print(f\"Confusion Matrix for {target}:\")\n",
        "#     print(conf_matrix_nb)\n",
        "#     # print(\"Classification Report:\")\n",
        "#     # print(class_report_nb)\n",
        "\n",
        "# # # Predict for new data\n",
        "# # docs_new = ['slice']\n",
        "# # processed_docs_new = [preprocess_text(doc) for doc in docs_new]\n",
        "\n",
        "# # for target in target_columns:\n",
        "# #     predicted = models[target].predict(processed_docs_new)\n",
        "# #     predicted_category = encoders[target].inverse_transform(predicted)\n",
        "# #     print(f\"Predicted {target} for '{docs_new[0]}': {predicted_category[0]}\")\n",
        "\n",
        "# # Define the new product names for which you want to predict the company, brand, and measurement type\n",
        "# docs_new = ['pepsi500']  # You can add more product names as needed\n",
        "# processed_docs_new = [preprocess_text(doc) for doc in docs_new]\n",
        "\n",
        "# # Loop over the target columns to make predictions for each\n",
        "# for target in target_columns:\n",
        "#     predicted = models[target].predict(processed_docs_new)\n",
        "#     predicted_category = encoders[target].inverse_transform(predicted)\n",
        "#     print(f\"Predicted {target} for '{docs_new[0]}': {predicted_category[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load and preprocess the data\n",
        "df = pd.read_csv(\"OpenItemAll.csv\")\n",
        "# df = df[~df['SubCategory'].isin(df['SubCategory'].value_counts()[df['SubCategory'].value_counts() < 2].index)]\n",
        "# df = df[~df['Category'].isin(df['Category'].value_counts()[df['Category'].value_counts() < 2].index)]\n",
        "df['SubCategory'] = df['SubCategory'].str.upper().str.strip().replace({'MEN DEODORANT': 'MEN DEODORANTS'})\n",
        "df['SubCategory'] = df['SubCategory'].str.upper().str.strip().replace({'SWEET CAKE': 'SWEET CAKES'})\n",
        "df['ProductName'] = df['ProductName'].fillna('')  # or df['ProductName'].astype(str)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['ProductName'] = df['ProductName'].apply(preprocess_text)\n",
        "\n",
        "# Define features and target columns\n",
        "X = df['ProductName']\n",
        "target_columns = ['SubCategory', 'Category', 'Company', 'Brand']\n",
        "\n",
        "# Prepare dictionaries to store results for each target column\n",
        "models = {}\n",
        "encoders = {}\n",
        "grid_search_results = {}\n",
        "\n",
        "# Directory to save the models and encoders\n",
        "save_dir = 'models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Loop over each target column\n",
        "for target in target_columns:\n",
        "    print(f\"\\nTraining model for {target}...\")\n",
        "\n",
        "    # Encode the target variable\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(df[target])\n",
        "\n",
        "    # Store the encoder for later use\n",
        "    encoders[target] = encoder\n",
        "\n",
        "    # Split the data\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the pipeline\n",
        "    pipeline_nb = Pipeline([\n",
        "        ('vect', TfidfVectorizer(analyzer='char', stop_words='english')),\n",
        "        ('clf', MultinomialNB()),\n",
        "    ])\n",
        "\n",
        "    # Define the parameter grid\n",
        "    param_grid = {\n",
        "        'vect__ngram_range': [(1, 3), (2, 4), (3, 5)],\n",
        "        'vect__use_idf': [True, False],\n",
        "        'clf__alpha': [0.1, 0.01, 0.001]\n",
        "    }\n",
        "\n",
        "    # Perform grid search\n",
        "    grid_search_nb = GridSearchCV(pipeline_nb, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "    grid_search_nb.fit(x_train, y_train)\n",
        "\n",
        "    # Store the trained model\n",
        "    models[target] = grid_search_nb\n",
        "    grid_search_results[target] = grid_search_nb.best_params_\n",
        "\n",
        "    # Save the model and encoder\n",
        "    joblib.dump(grid_search_nb, os.path.join(save_dir, f'{target}_model.pkl'))\n",
        "    joblib.dump(encoder, os.path.join(save_dir, f'{target}_encoder.pkl'))\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_nb = grid_search_nb.predict(x_test)\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "    precision_nb = precision_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "    recall_nb = recall_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "    f1_nb = f1_score(y_test, y_pred_nb, average='weighted', zero_division=1)\n",
        "    conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "    unique_classes = np.unique(np.concatenate((y_test, y_pred_nb)))\n",
        "    target_names = encoder.inverse_transform(unique_classes)\n",
        "    # class_report_nb = classification_report(y_test, y_pred_nb, labels=unique_classes, target_names=target_names, zero_division=1)\n",
        "\n",
        "    print(f\"Best parameters for {target}: \", grid_search_nb.best_params_)\n",
        "    print(f\"Accuracy for {target}: {accuracy_nb}\")\n",
        "    print(f\"Precision for {target}: {precision_nb}\")\n",
        "    print(f\"Recall for {target}: {recall_nb}\")\n",
        "    print(f\"F1 Score for {target}: {f1_nb}\")\n",
        "    print(f\"Confusion Matrix for {target}:\")\n",
        "    print(conf_matrix_nb)\n",
        "    # print(\"Classification Report:\")\n",
        "    # print(class_report_nb)\n",
        "\n",
        "# Define the new product names for which you want to predict the company, brand, and measurement type\n",
        "docs_new = ['pepsi500']  # You can add more product names as needed\n",
        "processed_docs_new = [preprocess_text(doc) for doc in docs_new]\n",
        "\n",
        "# Loop over the target columns to make predictions for each\n",
        "for target in target_columns:\n",
        "    model = joblib.load(os.path.join(save_dir, f'{target}_model.pkl'))\n",
        "    encoder = joblib.load(os.path.join(save_dir, f'{target}_encoder.pkl'))\n",
        "\n",
        "    predicted = model.predict(processed_docs_new)\n",
        "    predicted_category = encoder.inverse_transform(predicted)\n",
        "    print(f\"Predicted {target} for '{docs_new[0]}': {predicted_category[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-6cMrMO0Z_U",
        "outputId": "ffb5ba67-2e1b-4eb1-a06c-a55a4fc25502"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for SubCategory...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for SubCategory:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': False}\n",
            "Accuracy for SubCategory: 0.8356532849169525\n",
            "Precision for SubCategory: 0.8348256877124407\n",
            "Recall for SubCategory: 0.8356532849169525\n",
            "F1 Score for SubCategory: 0.8288257455616973\n",
            "Confusion Matrix for SubCategory:\n",
            "[[ 2  0  0 ...  0  0  0]\n",
            " [ 0  3  1 ...  0  0  0]\n",
            " [ 0  0  7 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 45]]\n",
            "\n",
            "Training model for Category...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Category:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': True}\n",
            "Accuracy for Category: 0.8597269854078408\n",
            "Precision for Category: 0.8545040616810186\n",
            "Recall for Category: 0.8597269854078408\n",
            "F1 Score for Category: 0.8543691050203744\n",
            "Confusion Matrix for Category:\n",
            "[[151   1   0 ...   1   0   8]\n",
            " [  1  10   0 ...   0   0   0]\n",
            " [  0   0  25 ...   0   0   0]\n",
            " ...\n",
            " [  4   0   0 ... 149   0   0]\n",
            " [  2   0   0 ...   1   9   0]\n",
            " [  6   0   0 ...   0   0 227]]\n",
            "\n",
            "Training model for Company...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Company:  {'clf__alpha': 0.1, 'vect__ngram_range': (3, 5), 'vect__use_idf': True}\n",
            "Accuracy for Company: 0.8890457938269114\n",
            "Precision for Company: 0.8903885242003067\n",
            "Recall for Company: 0.8890457938269114\n",
            "F1 Score for Company: 0.8658004306971294\n",
            "Confusion Matrix for Company:\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  1  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  1]\n",
            " [ 0  0  0 ...  0  0 27]]\n",
            "\n",
            "Training model for Brand...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Brand:  {'clf__alpha': 0.01, 'vect__ngram_range': (3, 5), 'vect__use_idf': False}\n",
            "Accuracy for Brand: 0.8818505816690202\n",
            "Precision for Brand: 0.8997593660895931\n",
            "Recall for Brand: 0.8818505816690202\n",
            "F1 Score for Brand: 0.880136233834143\n",
            "Confusion Matrix for Brand:\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 1  0  0 ...  0  0  0]\n",
            " [ 0  0  2 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 39]]\n",
            "Predicted SubCategory for 'pepsi500': CARBONATED SOFT DRINKS\n",
            "Predicted Category for 'pepsi500': BEVERAGES\n",
            "Predicted Company for 'pepsi500': PEPSICO\n",
            "Predicted Brand for 'pepsi500': PEPSI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_new = ['tapal danedar']  # You can add more product names as needed\n",
        "processed_docs_new = [preprocess_text(doc) for doc in docs_new]\n",
        "\n",
        "# Loop over the target columns to make predictions for each\n",
        "for target in target_columns:\n",
        "    predicted = models[target].predict(processed_docs_new)\n",
        "    predicted_category = encoders[target].inverse_transform(predicted)\n",
        "    print(f\"Predicted {target} for '{docs_new[0]}': {predicted_category[0]}\")"
      ],
      "metadata": {
        "id": "J5JLr0-5GTs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42f9fc5-59b0-4910-9f44-0d2de3340b87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted SubCategory for 'tapal danedar': TEA\n",
            "Predicted Category for 'tapal danedar': BEVERAGES\n",
            "Predicted Company for 'tapal danedar': TAPAL TEA\n",
            "Predicted Brand for 'tapal danedar': TAPAL DANEDAR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create a zip file of the models directory\n",
        "!zip -r /content/models.zip /content/models\n",
        "\n",
        "# Download the zip file to your local machine\n",
        "files.download('/content/models.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "fuJt0V51_Yvz",
        "outputId": "498b0a14-1782-4b08-ba0e-5a6f6c654e4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/SubCategory_model.pkl (deflated 99%)\n",
            "  adding: content/models/Brand_model.pkl (deflated 100%)\n",
            "  adding: content/models/SubCategory_encoder.pkl (deflated 47%)\n",
            "  adding: content/models/Brand_encoder.pkl (deflated 54%)\n",
            "  adding: content/models/Company_model.pkl (deflated 100%)\n",
            "  adding: content/models/Company_encoder.pkl (deflated 54%)\n",
            "  adding: content/models/Category_encoder.pkl (deflated 35%)\n",
            "  adding: content/models/Category_model.pkl (deflated 95%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0593d98c-a73f-4800-bac6-a221b410ab5a\", \"models.zip\", 22275015)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bdLHcT4_kVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}